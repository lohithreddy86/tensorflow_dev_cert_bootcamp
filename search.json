[
  {
    "objectID": "dl_and_tf_fundamentals.html",
    "href": "dl_and_tf_fundamentals.html",
    "title": "TF Fundamentals",
    "section": "",
    "text": "scalar = tf.constant(9)\nscalar.numpy()\n\n9\n\n\n\nscalar.ndim\n\n0\n\n\n\nCreating Tensors with tf.constant()\ntf.constant( value, dtype=None, shape=None, name=‘Const’ ) -&gt; Union[tf.Operation, ops._EagerTensorBase]\n=&gt; dtype helps to define the data type of the tensor - int64, float16 etc\n\nvector = tf.constant([1,2,3])\nvector\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\nvector.ndim\n\n1\n\n\n\nmatrix = tf.constant([[1,2,3],\n                      [5,8,9]])\nmatrix\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [5, 8, 9]], dtype=int32)&gt;\n\n\n\nmatrix.ndim\n\n2\n\n\n\ntensor = tf.constant([\n    [[1,2,3,9],\n     [4,5,6,4]],\n    [[2,5,3,4],\n     [4,5,8,7]],\n    [[9,6,4,2],\n     [3,1,2,1]]\n])\ntensor\n\n&lt;tf.Tensor: shape=(3, 2, 4), dtype=int32, numpy=\narray([[[1, 2, 3, 9],\n        [4, 5, 6, 4]],\n\n       [[2, 5, 3, 4],\n        [4, 5, 8, 7]],\n\n       [[9, 6, 4, 2],\n        [3, 1, 2, 1]]], dtype=int32)&gt;\n\n\n\ntensor.ndim\n\n3\n\n\n\n\nCreating Tensors with tf.variable()\ntf.Variable( initial_value=None, trainable=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, import_scope=None, constraint=None, synchronization=tf.VariableSynchronization.AUTO, aggregation=tf.compat.v1.VariableAggregation.NONE, shape=None, experimental_enable_variable_lifting=True )\nA variable maintains shared, persistent state manipulated by a program.\nThe Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n\nchangeable_tensor = tf.Variable([10,7])\nunchangeable_tensor = tf.constant([5,9])\nchangeable_tensor, unchangeable_tensor\n\n(&lt;tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([5, 9], dtype=int32)&gt;)\n\n\n\nWe can modify the elements of tensor using assign method\n\nchangeable_tensor[0].assign(6)\n\n&lt;tf.Variable 'UnreadVariable' shape=(2,) dtype=int32, numpy=array([6, 7], dtype=int32)&gt;\n\n\n\nunchangeable_tensor[0].assign(4)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign'\n\n\n\n\n\nCreating Random Tensors\n=&gt; Majorly used for the creation of tensors for weight initialization\n\nrandom_1 = tf.random.Generator.from_seed(16) # set seed for reproducibility\nrandom_1\n\n&lt;tensorflow.python.ops.stateful_random_ops.Generator at 0x7f49700b7370&gt;\n\n\n\nrandom_1 =random_1.normal(shape= (3,2))\nrandom_1\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-0.63509274,  0.3703566 ],\n       [-1.0939722 , -0.46014452],\n       [ 1.5420506 , -0.16822556]], dtype=float32)&gt;\n\n\n\nrandom_2  = tf.random.Generator.from_seed(16).normal(shape = (3,2))\nrandom_2\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-0.63509274,  0.3703566 ],\n       [-1.0939722 , -0.46014452],\n       [ 1.5420506 , -0.16822556]], dtype=float32)&gt;\n\n\n\nrandom_3  = tf.random.normal(shape = (3,2))\nrandom_3\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-0.5297089 ,  2.407835  ],\n       [-0.43932903, -0.14928319],\n       [ 0.77528226, -2.1867003 ]], dtype=float32)&gt;\n\n\nThe tf.random.Generator class The tf.random.Generator class is used in cases where you want each RNG call to produce different results. It maintains an internal state (managed by a tf.Variable object) which will be updated every time random numbers are generated. Because the state is managed by tf.Variable, it enjoys all facilities provided by tf.Variable such as easy checkpointing, automatic control-dependency and thread safety.\nThere are multiple ways to create a generator object. The easiest is Generator.from_seed, as shown above, that creates a generator from a seed. A seed is any non-negative integer. from_seed also takes an optional argument alg which is the RNG algorithm that will be used by this generator:\n\ng1 = tf.random.Generator.from_seed(16, alg = 'philox')\ng1 = g1.normal(shape=(3,4))\ng1\n\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[-0.63509274,  0.3703566 , -1.0939722 , -0.46014452],\n       [ 1.5420506 , -0.16822556, -0.43908644, -0.41292423],\n       [ 0.35877243, -1.9095894 , -0.20947689,  0.8286217 ]],\n      dtype=float32)&gt;\n\n\n\n\nShuffle the order of tensors\ntf.random.shuffle( value, seed=None, name=None )\nRandomly shuffles a tensor along its first dimension.\n\nnot_shuffled = tf.constant([[2,5,],\n                            [5,7],\n                            [9,3]])\nnot_shuffled\n\n&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[2, 5],\n       [5, 7],\n       [9, 3]], dtype=int32)&gt;\n\n\n\ntf.random.shuffle(not_shuffled)\n\n&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[5, 7],\n       [9, 3],\n       [2, 5]], dtype=int32)&gt;\n\n\nSets the global random seed.\ntf.random.set_seed( seed )\nOperations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\nIts interactions with operation-level seeds is as follows:\nIf neither the global seed nor the operation seed is set: A randomly picked seed is used for this op. If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both global and operation-level seeds explicitly. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.\n\ntf.random.set_seed(24)\ntf.random.shuffle(not_shuffled, seed=24)\n\n&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[9, 3],\n       [2, 5],\n       [5, 7]], dtype=int32)&gt;\n\n\n\n\nOther ways to make tensors\n\n# Create tensor of all ones\n\ntf.ones([10,7])\n\n&lt;tf.Tensor: shape=(10, 7), dtype=float32, numpy=\narray([[1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1.]], dtype=float32)&gt;\n\n\n\ntf.zeros(shape=(3,4))\n\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)&gt;\n\n\n\n\nTurn Numpy arrays into tensors\nThe main difference between numpy arrays and tensorflow tensors is that tensors can be run on a GPU computing\n\n# convert numpy arrays into tensors\nimport numpy as np \n\nnumpy_array = np.arange(1,25, dtype=np.int16)\nnumpy_array\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24], dtype=int16)\n\n\n\nconstant_tf = tf.constant(numpy_array, shape = (2,3,4))\nconstant_tf\n\n&lt;tf.Tensor: shape=(2, 3, 4), dtype=int16, numpy=\narray([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]], dtype=int16)&gt;\n\n\n\nvariable_tf = tf.Variable(numpy_array)\nvariable_tf\n\n&lt;tf.Variable 'Variable:0' shape=(24,) dtype=int16, numpy=\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24], dtype=int16)&gt;\n\n\n\n\nTensor attributes - to get more information from Tensors\nShape =&gt; tensor.shape\nRank =&gt; tensor.ndim\nAxis or Dimension =&gt; tensor[0], tensor[:,1]…\nSize =&gt; tf.size(tensor)\n\nrank_4_tensor = tf.zeros(shape = [2,3,4,5])\nrank_4_tensor\n\n&lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=\narray([[[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]]], dtype=float32)&gt;\n\n\n\nrank_4_tensor[0]\n\n&lt;tf.Tensor: shape=(3, 4, 5), dtype=float32, numpy=\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]], dtype=float32)&gt;\n\n\n\nrank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)\n\n(TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;)\n\n\n\nprint(\"Datatype of every element:\", rank_4_tensor.dtype)\nprint(\"Number of dimensions (rank):\", rank_4_tensor.ndim)\nprint(\"shape of tensor:\", rank_4_tensor.shape)\nprint(\"Elements along the zero axis:\", rank_4_tensor.shape[0])\nprint(\"Elements along the last axis:\", rank_4_tensor.shape[-1])\nprint(\"total number of elements in tensor:\", tf.size(rank_4_tensor).numpy())\n\nDatatype of every element: &lt;dtype: 'float32'&gt;\nNumber of dimensions (rank): 4\nshape of tensor: (2, 3, 4, 5)\nElements along the zero axis: 2\nElements along the last axis: 5\ntotal number of elements in tensor: 120\n\n\n\n\nIndexing and Expanding Tensors\n\n# fetching first two elements of the tensor in all dimensions\n\nrank_4_tensor[:1,:2,:1,:3]\n\n&lt;tf.Tensor: shape=(1, 2, 1, 3), dtype=float32, numpy=\narray([[[[0., 0., 0.]],\n\n        [[0., 0., 0.]]]], dtype=float32)&gt;\n\n\n\nrank_2_tensor = tf.reshape(rank_4_tensor, shape=(12,10))\nrank_2_tensor\n\n&lt;tf.Tensor: shape=(12, 10), dtype=float32, numpy=\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;\n\n\n\n# Add Extra Dimension to any tensor\n\nrank_3_tensor = rank_2_tensor[..., tf.newaxis]\nrank_3_tensor\n\n&lt;tf.Tensor: shape=(12, 10, 1), dtype=float32, numpy=\narray([[[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]]], dtype=float32)&gt;\n\n\n\n# Alternative to tf.newaxis - expanding axis in between\n\nchanged_tensor_new = tf.expand_dims(rank_2_tensor, axis = 1)\nchanged_tensor_new.shape\n\nTensorShape([12, 1, 10])\n\n\n\n\nTensor Operations\nBasic operations ‘+’, ‘-’, ’*‘,’/’\n\ntensor = tf.constant([[10,7],[4,9]])\ntensor\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 4,  9]], dtype=int32)&gt;\n\n\n\ntensor+10\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[20, 17],\n       [14, 19]], dtype=int32)&gt;\n\n\n\ntensor-1\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[9, 6],\n       [3, 8]], dtype=int32)&gt;\n\n\n\ntensor*2\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[20, 14],\n       [ 8, 18]], dtype=int32)&gt;\n\n\n\ntensor/2\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float64, numpy=\narray([[5. , 3.5],\n       [2. , 4.5]])&gt;\n\n\n\n# this might speed up operations when used with GPU\ntf.multiply(tensor, 10)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  70],\n       [ 40,  90]], dtype=int32)&gt;\n\n\n\n\nMatrix Multiplication\n\n# Dot product or Matrix multiplication\ntf.matmul(tensor, tensor)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[128, 133],\n       [ 76, 109]], dtype=int32)&gt;\n\n\n\n# Elementwise Multiplication\ntensor*tensor\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  49],\n       [ 16,  81]], dtype=int32)&gt;\n\n\n\ntensor@tensor\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[128, 133],\n       [ 76, 109]], dtype=int32)&gt;\n\n\n\ntf.tensordot(tensor, tensor, axes =1)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[128, 133],\n       [ 76, 109]], dtype=int32)&gt;\n\n\n\n\nChanging Data type in Tensors\n\nA = tf.constant([[10,7],\n                [5,2]])\nA\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 5,  2]], dtype=int32)&gt;\n\n\n\nB = tf.cast(A, dtype=tf.float16)\nB\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float16, numpy=\narray([[10.,  7.],\n       [ 5.,  2.]], dtype=float16)&gt;\n\n\n\n\nAggregations\n\nsample_tensor = tf.constant([[5,8],\n                             [-6,9]])\nsample_tensor\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5,  8],\n       [-6,  9]], dtype=int32)&gt;\n\n\n\ntf.abs(sample_tensor)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[5, 8],\n       [6, 9]], dtype=int32)&gt;\n\n\n\ntf.reduce_sum(sample_tensor).numpy()\n\n16\n\n\n\ntf.reduce_sum(tf.abs(sample_tensor)).numpy()\n\n28\n\n\n\ntf.reduce_mean(sample_tensor).numpy()\n\n4\n\n\n\ntf.reduce_min(sample_tensor).numpy()\n\n-6\n\n\n\ntf.reduce_max(sample_tensor).numpy()\n\n9\n\n\n\nstd_tensor = tf.constant([[5,8],\n                             [-6,9]], dtype=tf.float32)\nstd_tensor\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[ 5.,  8.],\n       [-6.,  9.]], dtype=float32)&gt;\n\n\n\nstd_dev = tf.math.reduce_std(std_tensor)\nstd_dev.numpy()\n\n5.9581876\n\n\n\nvar = tf.math.reduce_variance(std_tensor).numpy()\nvar\n\n35.5\n\n\n\n\nPositional Minimum and Maximum - argmin, argmax\n\n#create a tensor\npos_tensor = tf.random.Generator.from_seed(16).uniform(shape=[10])\npos_tensor\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([0.7631861 , 0.8340243 , 0.49447727, 0.6866319 , 0.300259  ,\n       0.26729417, 0.83389175, 0.62988555, 0.15143108, 0.47044265],\n      dtype=float32)&gt;\n\n\n\ntf.argmax(pos_tensor).numpy()\n\n1\n\n\n\npos_tensor[tf.argmax(pos_tensor)].numpy()\n\n0.8340243\n\n\n\ntf.argmin(pos_tensor).numpy()\n\n8\n\n\n\npos_tensor[tf.argmin(pos_tensor)].numpy()\n\n0.15143108\n\n\n\n\nSqueezing tensor\n\nsparse_tensor = tf.random.normal(shape=(1,1,1,1,50), dtype=tf.float32)\nsparse_tensor\n\n&lt;tf.Tensor: shape=(1, 1, 1, 1, 50), dtype=float32, numpy=\narray([[[[[ 2.665353  , -0.01305245,  3.2121453 ,  1.2325953 ,\n            1.3019717 , -0.01629734, -0.66601795, -1.0432147 ,\n           -1.7616948 , -1.6414421 ,  0.16661839, -0.99732476,\n            0.8872941 , -1.8125703 ,  0.0966946 , -0.2159489 ,\n            2.066713  ,  1.6013973 ,  0.48526224,  1.452654  ,\n            1.6297206 ,  0.8971428 ,  1.1796947 ,  0.95481825,\n            0.8847263 , -0.34303787,  0.86872697, -1.7373002 ,\n            0.16618411,  0.40596476,  0.73277277, -1.8712598 ,\n           -1.9836899 , -0.7865413 , -0.10133116,  0.47296703,\n           -0.06624011, -0.18455192,  0.6794147 ,  1.4783307 ,\n           -0.3054982 ,  0.96321213,  1.165348  , -1.5266519 ,\n           -0.17529055,  0.9577717 , -0.12913266,  1.0376912 ,\n           -1.1671338 ,  1.298102  ]]]]], dtype=float32)&gt;\n\n\n\nsqueezed_tensor = tf.squeeze(sparse_tensor)\nsqueezed_tensor, squeezed_tensor.shape\n\n(&lt;tf.Tensor: shape=(50,), dtype=float32, numpy=\n array([ 2.665353  , -0.01305245,  3.2121453 ,  1.2325953 ,  1.3019717 ,\n        -0.01629734, -0.66601795, -1.0432147 , -1.7616948 , -1.6414421 ,\n         0.16661839, -0.99732476,  0.8872941 , -1.8125703 ,  0.0966946 ,\n        -0.2159489 ,  2.066713  ,  1.6013973 ,  0.48526224,  1.452654  ,\n         1.6297206 ,  0.8971428 ,  1.1796947 ,  0.95481825,  0.8847263 ,\n        -0.34303787,  0.86872697, -1.7373002 ,  0.16618411,  0.40596476,\n         0.73277277, -1.8712598 , -1.9836899 , -0.7865413 , -0.10133116,\n         0.47296703, -0.06624011, -0.18455192,  0.6794147 ,  1.4783307 ,\n        -0.3054982 ,  0.96321213,  1.165348  , -1.5266519 , -0.17529055,\n         0.9577717 , -0.12913266,  1.0376912 , -1.1671338 ,  1.298102  ],\n       dtype=float32)&gt;,\n TensorShape([50]))\n\n\n\nOne hot encoding tensor\n\nsome_list = [1,2,3,4]\n\ntf.one_hot(some_list, depth =4)\n\n&lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 0., 0.]], dtype=float32)&gt;\n\n\n\ntf.config.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n\n\n\n!nvidia-smi\n\nSun Feb 18 02:18:32 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A6000               Off | 00000000:41:00.0 Off |                  Off |\n| 30%   37C    P8               7W / 300W |  47144MiB / 49140MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA RTX A6000               Off | 00000000:61:00.0 Off |                  Off |\n| 30%   36C    P8               7W / 300W |    266MiB / 49140MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      9385      C   ...anaconda3/envs/deep_lrnr/bin/python    47138MiB |\n|    1   N/A  N/A      9385      C   ...anaconda3/envs/deep_lrnr/bin/python      260MiB |\n+---------------------------------------------------------------------------------------+",
    "crumbs": [
      "TF Fundamentals"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tensorflow_dev_cert_bootcamp",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "tensorflow_dev_cert_bootcamp"
    ]
  },
  {
    "objectID": "index.html#git-commands",
    "href": "index.html#git-commands",
    "title": "tensorflow_dev_cert_bootcamp",
    "section": "GIT Commands",
    "text": "GIT Commands\ngit status =&gt; to find the uncommitted files in local git repo\ngit add . =&gt; to add all files from repo for commit git add  =&gt; for adding any specific file for commit\nsudo git commit -m “commit message” =&gt; this makes files ready for commit with comment\nsudo git push origin \nsudo git push =&gt; pushes changes and syncs the remote with local =&gt; this will ask git username and password post that commits the local to remote/online public repo",
    "crumbs": [
      "tensorflow_dev_cert_bootcamp"
    ]
  },
  {
    "objectID": "index.html#cookiecutter-steps",
    "href": "index.html#cookiecutter-steps",
    "title": "tensorflow_dev_cert_bootcamp",
    "section": "Cookiecutter steps",
    "text": "Cookiecutter steps\n\nnavigate to the folder where you want to create a new project eg - cd /home/dlvaayuai/storage/\nactivate the required conda environment eg - conda activate deep_lrnr\ncookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science answer questions project with template folder structure is created\ngit init\ngit add .\ngit status\ngit config –global user.email “lohithreddy86@gmail.com”\ngit config –global user.name “lohithreddy86”\ngit commit -m “Initial Commit with files and folder created through cookie cutter”\ngit branch -M main\nGo to github and create the new repository eg - tensorflow_dev_cert_bootcamp\ngit remote add origin https://github.com/lohithreddy86/tensorflow_dev_cert_bootcamp.git\ngit push -u origin main",
    "crumbs": [
      "tensorflow_dev_cert_bootcamp"
    ]
  },
  {
    "objectID": "index.html#how-to-generate-nbdev-docs",
    "href": "index.html#how-to-generate-nbdev-docs",
    "title": "tensorflow_dev_cert_bootcamp",
    "section": "How to Generate Nbdev docs",
    "text": "How to Generate Nbdev docs\nThe below steps helps to generate the documentation of the learnings as webpage and the same can be accessed through GIT for future refernces\nOnce the ML project template is created through cookiecutter\n\nRename the notebooks folder to nbs\nThen run command “nbdev_new” in terminal\nnbdev_new =&gt; This command would generate the necessary files within the project repository folder required for the - module build purposes and for documentation.\nfor document only repository - like practise or learning projects refer - https://nbdev.fast.ai/tutorials/docs_only.html\nrun below commands rm setup.py .github/workflows/test.yaml nbs/00_core.ipynb\nRemove your library folder (this will be the lib_path field in settings.ini)\nThen run command “nbdev_docs” in terminal\nThis command will generate html pages for all the jupyter notebooks.\n https://nbdev.fast.ai/api/quarto.html#nbdev_docs\npush the changes to git repository\nAccess webpage pertaining to a specific jupyter note book with github pages link appended with html page generated for the notebook. which can be checked in _docs folder in the repository.\nfor the html generated for the notebook “tensorflow_dev_cert_bootcamp/nbs/1_DL_and_TF_Fundamentals.ipynb” the _docs folder has html page “_docs/dl_and_tf_fundamentals.html” Hence we need to append “dl_and_tf_fundamentals” to the existing github repo pages which is “https://lohithreddy86.github.io/tensorflow_dev_cert_bootcamp/” as below.\n https://lohithreddy86.github.io/tensorflow_dev_cert_bootcamp/dl_and_tf_fundamentals",
    "crumbs": [
      "tensorflow_dev_cert_bootcamp"
    ]
  }
]